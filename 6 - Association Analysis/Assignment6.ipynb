{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name1: Peijie Yang\n",
    "EID1: py2554 \n",
    "\n",
    "\n",
    "Name2: Pengdi Xia\n",
    "EID2: px353"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Analysis\n",
    "\n",
    "Association analysis is one of the most used machine learning algorithms to extract hidden relationships from large datasets. In this assignment we'll be implementing two of the most commonly used algorithms for association rule mining - Apriori and FPTree algorithm.\n",
    "\n",
    "The dataset (`large_retail.txt`) that we are going to use for this assignment has been adapted from the [Retail Market Basket Dataset](http://fimi.ua.ac.be/data/retail.pdf). The dataset contains transactions records supplied by an anonymous Belgian retail supermarket store. Each line in the file represents a separate transaction with the item ids separated by space. The dataset has 3000 transaction records and 99 different item ids.\n",
    "\n",
    "We also provide a smaller dataset (`small_retail.txt`) with 9 transactions and 5 different item ids along with the solutions. *You should first test your implementation on this dataset, before running it on the larger dataset.*\n",
    "\n",
    "The assignment will be **autograded**, we will use the `diff` command in linux to compare the output files. So please **check your answers** based on the given sample output files.\n",
    "\n",
    "Implementation Hint:\n",
    "\n",
    "- Use the `frozenset` data structure in Python (similar to `set` in functionality) to represent the itemsets because `frozenset` is a hashable data structure. You can just maintain a dictionary that maps from the itemset to its support count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Apriori Algorithm\n",
    "\n",
    "Apriori algorithm is a classical algorithm in data mining. It is used for mining frequent itemsets and relevant association rules. In this part, you'll be implementing this algorithm for generating the itemsets that occur more than the `min_sup` threshold. Based on these frequent itemsets you'll find association rules that have confidence above the `min_conf` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports (you can add additional headers if you wish)\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the dataset from file\n",
    "def load_dataset(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        data = [[int(x) for x in line.rstrip().split()] for line in content]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 5],\n",
       " [2, 4],\n",
       " [2, 3],\n",
       " [1, 2, 4],\n",
       " [1, 3],\n",
       " [2, 3],\n",
       " [1, 3],\n",
       " [1, 2, 3, 5],\n",
       " [1, 2, 3]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset = load_dataset('small_retail.txt')\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Implement the function `createSet1` that takes input the entire dataset and returns the list of the set of size 1 itemsets. For example, for `small_retail.txt` it should return:\n",
    "~~~\n",
    "[frozenset({1}),\n",
    " frozenset({2}),\n",
    " frozenset({3}),\n",
    " frozenset({4}),\n",
    " frozenset({5})]\n",
    " ~~~\n",
    " Please **don't hardcode** the item ids, your code should support item ids that are non-sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSet1(dataset):\n",
    "    c1 = []\n",
    "    # your code goes here\n",
    "    temp = []\n",
    "    for i in range(len(dataset)):\n",
    "        for j in range(len(dataset[i])):\n",
    "            temp.append(dataset[i][j])\n",
    "    for k in set(temp):\n",
    "        temp2 =[]\n",
    "        temp2.append(k)\n",
    "        c1.append(frozenset(temp2))\n",
    "    return c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1}),\n",
       " frozenset({2}),\n",
       " frozenset({3}),\n",
       " frozenset({4}),\n",
       " frozenset({5})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createSet1(small_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Implement function `filter_candidates` that takes input the candidate itemsets, dataset and the minumum support count `min_sup` and filters out candidates that don't meet the support threshold.\n",
    "\n",
    "**Hint:** You should also return the support count information (perhaps as a `dict`) for the itemsets. This will be useful later on for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_candidates(candidate, dataset, min_sup):\n",
    "    retlist = []\n",
    "    support_data = []\n",
    "    indicate = True\n",
    "    # your code goes here\n",
    "    for i in range(len(candidate)):\n",
    "        count = 0\n",
    "        temp = []\n",
    "        for j in range(len(dataset)):\n",
    "            # subset operation\n",
    "            if set(candidate[i]) <= set(dataset[j]):\n",
    "                count += 1\n",
    "        if count >= min_sup:\n",
    "            retlist.append(candidate[i])\n",
    "            temp.append(candidate[i]) \n",
    "            temp.append(count)\n",
    "            support_data.append(temp)\n",
    "    return retlist, support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})] [[frozenset({1}), 6], [frozenset({2}), 7], [frozenset({3}), 6], [frozenset({4}), 2], [frozenset({5}), 2]]\n"
     ]
    }
   ],
   "source": [
    "freq1, sup1 = filter_candidates(createSet1(small_dataset), small_dataset, 2)\n",
    "print (freq1, sup1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Implement the function `generateNextItemsets` that takes in itemsets of size `k` and generates candidate itemsets of size `k + 1`.\n",
    "\n",
    "**Hint:** Use the fact that if `[1, 2, 3, 4]` is a frequent itemset of size 4 then `[1, 2, 3]` and `[1, 2, 4]` both will be frequent itemsets of size 3. You can use this to reduce the number of candidate itemsets that you need to check drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateNextItemsets(freq_sets):\n",
    "    retlist = []\n",
    "    # your code goes here\n",
    "    # find the last item\n",
    "    temp = []\n",
    "    for i in range(len(freq_sets)):\n",
    "        temp.append(max(list(freq_sets[i])))\n",
    "    # remove the duplicate value\n",
    "    temp2 = []\n",
    "    for m in set(temp):\n",
    "        temp2.append(m)\n",
    "    # compare part\n",
    "    for j in range(len(freq_sets)):\n",
    "        for k in range(len(temp2)):\n",
    "            if max(list(freq_sets[j])) < temp2[k]:\n",
    "                templist = list(freq_sets[j])\n",
    "                templist.append(temp2[k])\n",
    "                retlist.append(frozenset(templist))\n",
    "    return retlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1, 2}),\n",
       " frozenset({1, 3}),\n",
       " frozenset({1, 4}),\n",
       " frozenset({1, 5}),\n",
       " frozenset({2, 3}),\n",
       " frozenset({2, 4}),\n",
       " frozenset({2, 5}),\n",
       " frozenset({3, 4}),\n",
       " frozenset({3, 5}),\n",
       " frozenset({4, 5})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateNextItemsets(freq1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Implement the function `aprioriFreqItemsets` that takes the entire dataset as the input and returns the frequent itemsets that have support count more than `min_sup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aprioriFreqItemsets(dataset, min_sup):\n",
    "#     # your code goes here\n",
    "    retlist = []\n",
    "    supp_data =[]\n",
    "    init_set = createSet1(dataset)\n",
    "    init_retlist, init_supp = filter_candidates(init_set, dataset, min_sup)\n",
    "    retlist +=init_retlist\n",
    "    supp_data +=init_supp\n",
    "    templist = generateNextItemsets(init_retlist)\n",
    "    while templist != []:\n",
    "        templist, temp_data = filter_candidates(templist, dataset, min_sup)\n",
    "        if templist != []: \n",
    "            retlist +=templist\n",
    "            supp_data +=temp_data\n",
    "        templist = generateNextItemsets(templist)\n",
    "    return retlist, supp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retlist,supp_data= aprioriFreqItemsets(small_dataset,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Display the frequent item sets in the form of a table along with their `support_fraction` for the `large_retail.txt` dataset with min support count 300.\n",
    "\n",
    "Sample Table Format (tab separated table)\n",
    "\n",
    "~~~\n",
    "Sup     Freq Itemset\n",
    "0.67\t[1]\n",
    "0.44\t[1, 2]\n",
    "(and so on)\n",
    "...\n",
    "...\n",
    "~~~\n",
    "\n",
    "`support_fraction(itemset) = support_count(itemset) / num_total_transactions`.\n",
    "\n",
    "Note that the `support_fraction` should be rounded to the nearest 2 decimal places (use `round(sup, 2)`). Also `support_fraction` and the itemset should be separated by a tab (`'\\t'`). The itemsets should also be in a sorted order where smaller itemsets should come before larger itemsets and itemsets of the same size should be sorted amongst themselves.\n",
    "\n",
    "For eg. \n",
    "~~~~\n",
    "[1, 2] should come before [1, 2, 3]\n",
    "[1, 2, 3] should come before [1, 2, 4]\n",
    "[1, 2, 3] should come before [1, 4, 5]\n",
    "[1, 2, 3] should come before [2, 3, 4]\n",
    "~~~~\n",
    "\n",
    "Note that **this order is very important** because your output will be checked using the `diff` command. The output also **shouldn't contain any duplicates**. The sample output for the `small_retail.txt` dataset with min support count as 2 is:\n",
    "\n",
    "~~~~\n",
    "Sup     Freq Itemset\n",
    "0.67\t[1]\n",
    "0.78\t[2]\n",
    "0.67\t[3]\n",
    "0.22\t[4]\n",
    "0.22\t[5]\n",
    "0.44\t[1, 2]\n",
    "0.44\t[1, 3]\n",
    "0.22\t[1, 5]\n",
    "0.44\t[2, 3]\n",
    "0.22\t[2, 4]\n",
    "0.22\t[2, 5]\n",
    "0.22\t[1, 2, 3]\n",
    "0.22\t[1, 2, 5]\n",
    "~~~~\n",
    "\n",
    "**Store** this output for the `large_retail.txt` dataset in the file `apriori_itemsets.txt`. The sample output file for the `small_retail.txt` dataset has been provided to you as `small_apriori_itemsets.txt` for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "large_dataset = load_dataset('large_retail.txt')\n",
    "retlist,retmap = aprioriFreqItemsets(large_dataset,300)\n",
    "file1 = open(\"apriori_itemsets.txt\",\"w\") \n",
    "file1.write(\"Sup\\tFreq Itemset\")  \n",
    "for i in range(len(retmap)):\n",
    "    file1.write(\"\\n\"+str(round(retmap[i][1]/len(large_dataset),2))+\"\\t\"+str(sorted(list(retmap[i][0]), key=int)))   \n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Display the closed frequent item sets along with their `support_fraction` in the same format as specified in Q5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[frozenset({31}), 309], [frozenset({32}), 691], [frozenset({36}), 320], [frozenset({38}), 771], [frozenset({39}), 1793], [frozenset({41}), 938], [frozenset({48}), 1396], [frozenset({60}), 337], [frozenset({65}), 329], [frozenset({89}), 343], [frozenset({32, 39}), 420], [frozenset({32, 48}), 370], [frozenset({38, 39}), 520], [frozenset({41, 38}), 323], [frozenset({48, 38}), 384], [frozenset({41, 39}), 697], [frozenset({48, 39}), 982], [frozenset({48, 41}), 534], [frozenset({48, 41, 39}), 428]]\n",
      "Sup\tFreq Itemset\n",
      "0.1\t[31]\n",
      "0.23\t[32]\n",
      "0.11\t[36]\n",
      "0.26\t[38]\n",
      "0.6\t[39]\n",
      "0.31\t[41]\n",
      "0.47\t[48]\n",
      "0.11\t[60]\n",
      "0.11\t[65]\n",
      "0.11\t[89]\n",
      "0.14\t[32, 39]\n",
      "0.12\t[32, 48]\n",
      "0.17\t[38, 39]\n",
      "0.11\t[38, 41]\n",
      "0.13\t[38, 48]\n",
      "0.23\t[39, 41]\n",
      "0.33\t[39, 48]\n",
      "0.18\t[41, 48]\n",
      "0.14\t[39, 41, 48]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "temp_list = []\n",
    "for i in range(len(retlist)):\n",
    "    temp_list.append(retlist[i])\n",
    "print(retmap)\n",
    "print(\"Sup\\tFreq Itemset\")\n",
    "for i in range(len(temp_list)):\n",
    "    write = True\n",
    "    for j in range(len(temp_list)):\n",
    "        if temp_list[i] < temp_list[j] and retmap[i][1] == retmap[j][1]:\n",
    "            write = False\n",
    "    if write:\n",
    "        print(str(round(retmap[i][1]/len(large_dataset),2))+\"\\t\"+str(sorted(list(retmap[i][0]), key=int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. (EXTRA CREDIT +5 points)** Generate the rules having confidence above `min_conf = 0.5` using the frequent itemsets generated in Q5. Display the rules in the form of a table.\n",
    "\n",
    "Sample table format (tab separated table)\n",
    "\n",
    "~~~\n",
    "Sup     Conf    Rule\n",
    "0.44\t0.67\t[1] -> [2]\n",
    "0.22\t1.0\t [5] -> [1, 2]\n",
    "0.22\t1.0\t [2, 5] -> [1]\n",
    "(and so on)\n",
    "...\n",
    "...\n",
    "~~~\n",
    "\n",
    "Note that rule confidence should be rounded to the nearest 2 decimal places (use `round(conf, 2)`). This table should also be tab (`'\\t'`) separated. The rules should be displayed in the sorted order. If a rule is given as `LHS -> RHS` then the rules for which `len(LHS)` is lesser should appear first. If the `len(LHS)` is equal for two rules then rules for which `len(RHS)` is lesser should appear first. If both `len(LHS)` and `len(RHS)` is equal then the rules should be sorted based on LHS first and then based on RHS.\n",
    "\n",
    "~~~~\n",
    "Note:\n",
    "LHS (Left Hand Side)\n",
    "RHS (Right Hand Side)\n",
    "~~~~\n",
    "\n",
    "For eg.\n",
    "~~~~\n",
    "[3] -> [2] should come before [1, 3] -> [4]\n",
    "[4] -> [2] should come before [2] -> [3, 4]\n",
    "[1, 3] -> [2] should come before [1, 5] -> [2]\n",
    "[1, 2] -> [3] should come before [1, 2] -> [5]\n",
    "~~~~\n",
    "\n",
    "Note that **this order is very important** because your output will be checked using the `diff` command. The sample output for the `small_retail.txt` dataset with `min_conf = 0.5` is:\n",
    "\n",
    "~~~~\n",
    "Sup\t Conf\tRule\n",
    "0.44\t0.67\t[1] -> [2]\n",
    "0.44\t0.67\t[1] -> [3]\n",
    "0.44\t0.57\t[2] -> [1]\n",
    "0.44\t0.57\t[2] -> [3]\n",
    "0.44\t0.67\t[3] -> [1]\n",
    "0.44\t0.67\t[3] -> [2]\n",
    "0.22\t1.0\t [4] -> [2]\n",
    "0.22\t1.0\t [5] -> [1]\n",
    "0.22\t1.0\t [5] -> [2]\n",
    "0.22\t1.0\t [5] -> [1, 2]\n",
    "0.22\t0.5\t [1, 2] -> [3]\n",
    "0.22\t0.5\t [1, 2] -> [5]\n",
    "0.22\t0.5\t [1, 3] -> [2]\n",
    "0.22\t1.0\t [1, 5] -> [2]\n",
    "0.22\t0.5\t [2, 3] -> [1]\n",
    "0.22\t1.0\t [2, 5] -> [1]\n",
    "~~~~\n",
    "\n",
    "**Store** this output for the `large_retail.txt` dataset in the file `apriori_rules.txt`. The sample output file for the `small_retail.txt` dataset has been provided to you as `small_apriori_rules.txt` for your convenience.\n",
    "\n",
    "**Hint:** you don't actually need to traverse the entire dataset to compute the confidence for a rule since you have already computed the `support_data` for all the frequent itemsets. `conf(LHS -> RHS) = sup(LHS union RHS) / sup(LHS)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# build a dictrionary base on sup_data\n",
    "def dic (supp_data):\n",
    "    result = {}\n",
    "    for i in range(len(supp_data)):\n",
    "        result[supp_data[i][0]] = supp_data[i][1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({31}), 309],\n",
       " [frozenset({32}), 691],\n",
       " [frozenset({36}), 320],\n",
       " [frozenset({38}), 771],\n",
       " [frozenset({39}), 1793],\n",
       " [frozenset({41}), 938],\n",
       " [frozenset({48}), 1396],\n",
       " [frozenset({60}), 337],\n",
       " [frozenset({65}), 329],\n",
       " [frozenset({89}), 343],\n",
       " [frozenset({32, 39}), 420],\n",
       " [frozenset({32, 48}), 370],\n",
       " [frozenset({38, 39}), 520],\n",
       " [frozenset({38, 41}), 323],\n",
       " [frozenset({38, 48}), 384],\n",
       " [frozenset({39, 41}), 697],\n",
       " [frozenset({39, 48}), 982],\n",
       " [frozenset({41, 48}), 534],\n",
       " [frozenset({39, 41, 48}), 428]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retlist,sup_data = aprioriFreqItemsets(large_dataset, 300)\n",
    "sup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# your code goes here\n",
    "result = []\n",
    "dictionary = dic(sup_data)\n",
    "element = []\n",
    "# loop over the freq itemset\n",
    "for i in range(len(sup_data)):\n",
    "    temp = []\n",
    "    lhs = []\n",
    "    if len(sup_data[i][0]) == 2 :\n",
    "        temp.append(i)\n",
    "        lhs.append(sorted(list(sup_data[i][0]), key=int)[0])\n",
    "        temp.append(lhs)\n",
    "        temp.append(list(set(sup_data[i][0])-set(lhs)))\n",
    "        element.append(temp) \n",
    "    elif len(sup_data[i][0]) == 3 :\n",
    "        for j in range(len(list(sup_data[i][0]))):\n",
    "            lhs = []\n",
    "            temp = []\n",
    "            temp.append(i)\n",
    "            lhs.append(sorted(list(sup_data[i][0]), key=int)[j])\n",
    "            temp.append(lhs)\n",
    "            temp.append(sorted(list(set(sup_data[i][0])-set(lhs)), key=int))\n",
    "            element.append(temp)\n",
    "# fill it up\n",
    "length = len(element)\n",
    "for k in range(length):\n",
    "    temp = []\n",
    "    lhs = element[k][2]\n",
    "    rhs = element[k][1]\n",
    "    temp.append(element[k][0])\n",
    "    temp.append(lhs)\n",
    "    temp.append(rhs)\n",
    "    element.append(temp)\n",
    "    \n",
    "# fliter\n",
    "for k in range(len(element)):\n",
    "    conf=round(sup_data[element[k][0]][1]/dictionary[frozenset(element[k][1])],2)\n",
    "    if conf >=0.5:\n",
    "        element[k].append(conf)\n",
    "        result.append(element[k])\n",
    "result = sorted(result, key=itemgetter(1,2))\n",
    "result = sorted(result, key= lambda result: len(result[1]))\n",
    "#write into file\n",
    "file2 = open(\"apriori_rules.txt\",\"w\") \n",
    "file2.write(\"Sup\\tConf\\tRule\")  \n",
    "for i in range(len(result)):\n",
    "    file2.write(\"\\n\"+str(round(sup_data[result[i][0]][1]/len(large_dataset),2))\n",
    "                +\"\\t\"+str(result[i][3])\n",
    "                +\"\\t\"+str(result[i][1]) +\" -> \"+str(result[i][2]))\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Frequent Pattern Tree\n",
    "\n",
    "The FP-Growth Algorithm, proposed by [Han](https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf), is an efficient and scalable method for mining the complete set of frequent patterns by pattern fragment growth, using an extended prefix-tree structure for storing compressed and crucial information about frequent patterns named frequent-pattern tree (FP-tree). [wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables:\n",
    "# self.item_id: item id of the item\n",
    "# self.item_count: item count for this node\n",
    "# self.node_link: next pointer for the linked list that connects\n",
    "#                 nodes of the same item_id (required for the FP-growth algorithm)\n",
    "# self.parent: pointer to the parent node (should be None for the root)\n",
    "# self.children: dictionary for the children (maps from item_id to the FPTreeNode object)\n",
    "# NOTE: root node should have item_id as -1\n",
    "\n",
    "class FPTreeNode:\n",
    "    def __init__(self, uid, num):\n",
    "        self.item_id = uid\n",
    "        self.item_count = num\n",
    "        self.node_link = None\n",
    "        self.parent = None\n",
    "        self.children = {}\n",
    "    def displayTree(self, tab=1):\n",
    "        if self.item_id == -1:\n",
    "            print ('  '*tab, 'root')\n",
    "        else:\n",
    "            print ('  '*tab, 'item_id:', self.item_id, 'item_count:', self.item_count)\n",
    "        for key in sorted(self.children.keys()):\n",
    "            self.children[key].displayTree(tab + 1)\n",
    "    # helper function for saveToFile\n",
    "    def saveToFile_helper(self, fp, tab=1):\n",
    "        if self.item_id == -1:\n",
    "            print ('  '*tab, 'root', file=fp)\n",
    "        else:\n",
    "            print ('  '*tab, 'item_id:', self.item_id, 'item_count:', self.item_count, file=fp)\n",
    "        for key in sorted(self.children.keys()):\n",
    "            self.children[key].saveToFile_helper(fp, tab + 1)\n",
    "    # call this to save to file\n",
    "    def saveToFile(self, filename):\n",
    "        with open(filename, 'w') as fp:\n",
    "            self.saveToFile_helper(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Build the FP-Tree for the `large_retail.txt` dataset with minimum support count as 300 and display the tree using the `displayTree` function. Also save this output to `fp_tree.txt` using the `saveToFile` function.\n",
    "\n",
    "Note that while reordering the transactions based on their frequency, **item_ids that have equal frequency should be ordered based on the item_id value**.\n",
    "\n",
    "For eg.\n",
    "~~~~\n",
    "For the small_retail.txt dataset:\n",
    "{item_id: frequency} -> {1: 6, 2: 7, 3: 6, 4: 2, 5: 2}\n",
    "The transaction [1, 2, 3, 5] should be reordered as [2, 1, 3, 5]\n",
    "Notice the relative ordering of 1 and 3 (both have 6 occurences in the dataset)\n",
    "~~~~\n",
    "\n",
    "Note that the **relative order between transactions should not be changed**, they should be inserted in the same order as they appear in the dataset. For the `small_retail.txt` the transactions would be inserted in the FP Tree in this order:\n",
    "~~~~\n",
    "[2, 1, 5]\n",
    "[2, 4]\n",
    "[2, 3]\n",
    "[2, 1, 4]\n",
    "[1, 3]\n",
    "[2, 3]\n",
    "[1, 3]\n",
    "[2, 1, 3, 5]\n",
    "[2, 1, 3]\n",
    "~~~~\n",
    "\n",
    "The tree output for `small_retail.txt` dataset is given as follows:\n",
    "~~~~\n",
    "   root\n",
    "     item_id: 1 item_count: 2\n",
    "       item_id: 3 item_count: 2\n",
    "     item_id: 2 item_count: 7\n",
    "       item_id: 1 item_count: 4\n",
    "         item_id: 3 item_count: 2\n",
    "           item_id: 5 item_count: 1\n",
    "         item_id: 4 item_count: 1\n",
    "         item_id: 5 item_count: 1\n",
    "       item_id: 3 item_count: 2\n",
    "       item_id: 4 item_count: 1\n",
    "~~~~\n",
    "This output has been provided to you as `small_fp_tree.txt` for your convenience. You can use the `diff` command in Linux to check your output with the provided output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "import operator\n",
    "min_sup = 300\n",
    "small_dataset = load_dataset('large_retail.txt')\n",
    "frequency_set = {}\n",
    "\n",
    "for x in small_dataset: \n",
    "    for y in x:\n",
    "        if y not in frequency_set:\n",
    "            frequency_set[y] = 1\n",
    "        else: \n",
    "            frequency_set[y] += 1\n",
    "sorted_set = sorted(frequency_set.items(), key=lambda x: (x[1] ,-x[0]), reverse=True)\n",
    "#sorted_set = {2: 7, 1: 6, 3: 6, 4: 2, 5: 2}\n",
    "#put sorted keys into sort_set = {2,1,3,4,5}\n",
    "sort_set = []\n",
    "for x in sorted_set:\n",
    "    if x[1] >= min_sup:\n",
    "        sort_set.append(x[0])\n",
    "new_small_set = []\n",
    "for x in small_dataset:\n",
    "    temp = []\n",
    "    for y in x:\n",
    "        if y in sort_set:\n",
    "            temp.append(y)\n",
    "    if len(temp) != 0:\n",
    "        new_small_set.append(temp)\n",
    "\n",
    "new_freq_set={}\n",
    "count = len(sort_set)\n",
    "for x in sort_set:\n",
    "    new_freq_set[x] = count\n",
    "    count -= 1\n",
    "\n",
    "new_data_set = []\n",
    "for x in new_small_set:\n",
    "    temp = {}\n",
    "    new_set = []\n",
    "    for y in x:\n",
    "        temp[y] = new_freq_set[y]\n",
    "    #sorted by weight assigned in new_freq_set\n",
    "    temp2 = sorted(temp.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    for z in temp2:\n",
    "        new_set.append(z[0])\n",
    "    new_data_set.append(new_set)\n",
    "\n",
    "\n",
    "\n",
    "def leaf(uid,  parent_node, count):\n",
    "    leafNode = FPTreeNode(-1,1)\n",
    "    leafNode.item_id = uid\n",
    "    leafNode.item_count = count\n",
    "    leafNode.parent = parent_node\n",
    "    leafNode.children = {}\n",
    "    return leafNode\n",
    "\n",
    "def sublist (data):\n",
    "    sublist_result = {}\n",
    "    uid_dict = {}\n",
    "    for x in data: \n",
    "        if x[0] in sublist_result:\n",
    "            if len(x) > 1:\n",
    "                sublist_result[x[0]].append(x[1:])\n",
    "        else:\n",
    "            sublist_result[x[0]] = [x[1:]]\n",
    "        if x[0] in uid_dict:\n",
    "            uid_dict[x[0]] += 1\n",
    "        else:\n",
    "            uid_dict[x[0]] = 1\n",
    "    for i in sublist_result:\n",
    "        for j in sublist_result[i]:\n",
    "            if len(j) == 0 and len(sublist_result[i]) != 1:\n",
    "                sublist_result[i].remove(j)\n",
    "    return sublist_result, uid_dict\n",
    "\n",
    "firstNode = {} \n",
    "nodeLink = {}\n",
    "def createTree(data, parent_node,firstNode,nodeLink):\n",
    "    sublist_result, uid_dict = sublist (data)\n",
    "    for x in sublist_result:\n",
    "        node = FPTreeNode(x,0)\n",
    "        if len(sublist_result[x][0]) == 0:\n",
    "            node = leaf(x, parent_node, uid_dict[x])\n",
    "            parent_node.children[x] = node\n",
    "        else:\n",
    "            node = FPTreeNode(x,uid_dict[x]) \n",
    "            node.parent = parent_node\n",
    "            parent_node.children[x] = node\n",
    "            createTree(sublist_result[x], node,firstNode,nodeLink)\n",
    "        if x not in firstNode:\n",
    "            firstNode[x] = node\n",
    "            nodeLink[x] = node\n",
    "        else:\n",
    "            nodeLink[x].node_link = node\n",
    "            nodeLink[x] = nodeLink[x].node_link\n",
    "    return parent_node\n",
    "\n",
    "myTree = createTree(new_data_set, FPTreeNode(-1,1),firstNode,nodeLink)\n",
    "# myTree.displayTree()\n",
    "myTree.saveToFile('fp_tree.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Implement the FP-growth algorithm to generate the frequent itemsets from the FP-tree you generate in Q7 (with min support count of 300). Display the frequent item sets in the same format as specified in Q5. Store this output for the large_retail.txt dataset in the file fp_growth_itemsets.txt. Remember you are only allowed to mine the FP tree that you generate in Q7. you cannot use the dataset as the input to your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPGrowth(root, cur, minsup):    \n",
    "    if root.item_id != -1:\n",
    "        cur.append(root.item_id)\n",
    "        first_node = firstNode[root.item_id]\n",
    "        count = 0\n",
    "        while first_node != None:\n",
    "            temp2 = first_node\n",
    "            temp = []\n",
    "            while temp2.item_id != -1:\n",
    "                temp.append(temp2.item_id)\n",
    "                temp2 = temp2.parent\n",
    "            if set(cur).issubset(set(temp)): \n",
    "                count += first_node.item_count\n",
    "            first_node = first_node.node_link\n",
    "        if count >= minsup:\n",
    "            key = frozenset(cur)\n",
    "            freq_itemset[key] = count        \n",
    "    for child in root.children.values():\n",
    "        FPGrowth(child, cur, minsup) \n",
    "    if root.item_id != -1:\n",
    "        cur.remove(root.item_id)\n",
    "            \n",
    "\n",
    "freq_itemset = {}\n",
    "FPGrowth(myTree, [], 300)\n",
    "temp_list = []\n",
    "retmap = []\n",
    "\n",
    "file4 = open('fp_growth_itemsets.txt', 'w')\n",
    "freq_dict = {}\n",
    "for x in freq_itemset:\n",
    "    temp_list.append(x)\n",
    "    retmap.append([x,freq_itemset[x]])\n",
    "# print(retmap)\n",
    "freq_sorted_list = []\n",
    "for x in range(len(temp_list)):\n",
    "    freq_sorted_list.append(((round(retmap[x][1]/len(small_dataset),2)),sorted(list(retmap[x][0]), key=int)))\n",
    "# print(freq_sorted_list)\n",
    "\n",
    "sorted_set = sorted(freq_sorted_list, key=lambda x: (len(x[1]), x[1]), reverse=False)\n",
    "file4.write('Sup\\tFreq Itemset')\n",
    "for x in sorted_set:\n",
    "    file4.write('\\n' + str(x[0]) + \"\\t\"+str(x[1]))\n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
